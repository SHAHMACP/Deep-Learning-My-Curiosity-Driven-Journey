{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following Python code demonstrates how to use a simple Convolutional Neural Network (CNN) to classify images from the sklearn digits dataset using TensorFlow and Keras. The digits dataset This dataset contains 8×8 pixel images of handwritten digits (0 through 9), which is a good size for a simple CNN demonstration."
      ],
      "metadata": {
        "id": "34Kqu6Aj2SSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Characteristics:\n",
        "- 1,797 samples of 8×8 images, each representing a handwritten digit.\n",
        "- Target Classes: 10 (digits from 0 to 9).\n",
        "- Image Size: 8×8 pixels."
      ],
      "metadata": {
        "id": "qMotrfPa2WM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the digits dataset is relatively simple and small, especially compared to typical datasets used for CNNs (like MNIST or CIFAR-10), it can still be useful for illustrating the basic principles of how a CNN works.\n",
        "\n",
        "\n",
        "Note that the performance of a CNN on such a small and simple dataset may not be as impressive or illustrative as it would be on larger, more complex datasets. For more advanced or realistic demonstrations of CNN capabilities, datasets like MNIST (handwritten digits), CIFAR-10 (60,000 32×32 color images in 10 classes), or ImageNet (large dataset for more complex image recognition tasks) are generally preferred."
      ],
      "metadata": {
        "id": "V3IJiESf2dbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D"
      ],
      "metadata": {
        "id": "Uj22ZMUc2dLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.images, digits.target\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwduX6bju6AB",
        "outputId": "92a681a1-4947-4f45-9435-9ea685a71c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
              "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
              "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
              "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
              "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
              "\n",
              "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
              "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
              "\n",
              "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
              "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
              "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
              "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
              "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
              "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
              "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
              "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
              "\n",
              "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
              "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
              "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
              "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
              "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
              "\n",
              "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
              "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
              "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
              "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
              "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Normalizing pixel values\n",
        "X = X / 16.0"
      ],
      "metadata": {
        "id": "NTgXR6qtu_X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the data to fit the model\n",
        "# CNN in Keras requires an extra dimension at the end for channels,\n",
        "# and the digits images are grayscale so it's just 1 channel\n",
        "X = X.reshape(-1, 8, 8, 1)"
      ],
      "metadata": {
        "id": "8cFHD7p1vAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to categorical (one-hot encoding)\n",
        "y = to_categorical(y)"
      ],
      "metadata": {
        "id": "OGMimHI1vFyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "quIkzzLkvHqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a simple CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(8, 8, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))  # 10 classes for digits 0-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MRxD4V6vKe5",
        "outputId": "f60baab0-fa12-4ca3-f48f-a6b99a113a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yLmBFCQsvP-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxHpPXoCvRYK",
        "outputId": "0f8c5bb0-2c52-479e-ffa6-e7d3891c2bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.2945 - loss: 2.2109 - val_accuracy: 0.7306 - val_loss: 1.7102\n",
            "Epoch 2/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7892 - loss: 1.4503 - val_accuracy: 0.8917 - val_loss: 0.7263\n",
            "Epoch 3/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9149 - loss: 0.6052 - val_accuracy: 0.9083 - val_loss: 0.3713\n",
            "Epoch 4/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.3595 - val_accuracy: 0.9250 - val_loss: 0.2670\n",
            "Epoch 5/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9434 - loss: 0.2436 - val_accuracy: 0.9556 - val_loss: 0.2008\n",
            "Epoch 6/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9585 - loss: 0.1999 - val_accuracy: 0.9694 - val_loss: 0.1794\n",
            "Epoch 7/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9680 - loss: 0.1588 - val_accuracy: 0.9583 - val_loss: 0.1607\n",
            "Epoch 8/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9698 - loss: 0.1307 - val_accuracy: 0.9722 - val_loss: 0.1345\n",
            "Epoch 9/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.1237 - val_accuracy: 0.9667 - val_loss: 0.1400\n",
            "Epoch 10/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.1289 - val_accuracy: 0.9694 - val_loss: 0.1078\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dfbc38d2780>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xn_fjET2Gey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c343576-eb9a-441d-9085-81bd676fdeb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9690 - loss: 0.1027 \n",
            "0.9694444537162781\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "accuracy = model.evaluate(X_test, y_test)[1]\n",
        "print(accuracy)"
      ]
    }
  ]
}